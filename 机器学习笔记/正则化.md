正则化
============

正则化是一种回归的形式，它将系数估计（coefficient estimate）朝零的方向进行约束、调整或缩小。也就是说，正则化可以在学习过程中降低模型复杂度和不稳定程度，从而避免过拟合的危险。


# 1 范数
范数是衡量某个向量空间（或矩阵）中的每个向量的长度或大小。范数的一般化定义：对实数p>=1， 范数定义如下：

$$
\|x\|_ {p}=\left(\sum_{i=1}^{n}\left|x_{i}\right|^{p}\right)^{\frac{1}{p}}
$$

### L1范数
当p=1时，是L1范数，其表示某个向量中所有元素绝对值的和。
### L2范数
当p=2时，是L2范数， 表示某个向量中所有元素平方和再开根， 也就是欧几里得距离公式。


$$
\begin{aligned}
&\Omega(\theta)=|| \theta||_ {2}^{2}=\sum_{m=1}^{M} \theta_{m}^{2} \\
&\min _ {\theta} \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}\left(y_{i}, f_{\theta}\left(x_{i}\right)\right)+\lambda\|\theta\|_ {2}^{2}
\end{aligned}
$$


## 总结
* L2 regularizer ：使得模型的解偏向于范数较小的 W，通过限制 W 范数的大小实现了对模型空间的限制，从而在一定程度上避免了 overfitting 。不过 ridge regression 并不具有产生稀疏解的能力，得到的系数仍然需要数据中的所有特征才能计算预测结果，从计算量上来说并没有得到改观。  
* L1 regularizer ：它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。

## 参考
https://www.jianshu.com/p/c9bb6f89cfcc   
https://blog.csdn.net/qq_25847123/article/details/90400678  
https://www.boyuai.com/  

